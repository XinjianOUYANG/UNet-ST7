{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pratical Session U-Nets",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XinjianOUYANG/UNet-ST7/blob/master/Pratical_Session_U_Nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKPAVGu-9c-W"
      },
      "source": [
        "# Practical Session: Tumor Segmentation with U-nets\n",
        "\n",
        "**Group**: Molin LIU,Mickeal, Xinjian OUYANG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktdOQdxv-Hoa"
      },
      "source": [
        "## Problem Description\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGHkBzBt_cwz"
      },
      "source": [
        "In medical diagnosis, it is of great value to identify tumor's posiion and size by CT image, which could further help the doctor to judge if the tumor transfer happens or not. This practical session aims to segment the tumor area from existing CT images with the help of U-nets. \n",
        "\n",
        "Here is a graph to help you understand better: the photo in the left is the original CT image, the photo in the right is the tumor area hand-written by the doctor. \n",
        "\n",
        "The segmentation requirements are:\n",
        "\n",
        "\n",
        "*   segment the tumor area as accurate as possible;\n",
        "*   The no-tumor area in the CT image should be presented as **black**;\n",
        "\n",
        "![Tumor](https://github.com/XinjianOUYANG/UNet-ST7/blob/master/tumor.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqTEsa1MPeNd"
      },
      "source": [
        "## U-Nets introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh15hF6EVckY"
      },
      "source": [
        "As presented in the 5-minutes video, U-Nets is a specific fully convolutional network (FCN), which consists of a contracting path and an expansive path as shown in the graph. U-net is widely used for fast and precise segmentation of images thanks to its special structure. \n",
        "\n",
        "If you forget our video presentation, don't worry. This link provides you with further explanation about [U-Nets](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).\n",
        "\n",
        "![picture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy3Zjwju-Zzg"
      },
      "source": [
        "## Basic Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQDRZ2CKO67g"
      },
      "source": [
        "This practical session follows a typical deep learning processus:\n",
        "\n",
        "*   Data Preprocessing;\n",
        "*   Network Construction;\n",
        "*   Compiling Network;\n",
        "*   Training Network;\n",
        "*   Test and Evaluation;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_thivaTCVUyV"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNuCmZki_rjY"
      },
      "source": [
        "* Data preprocessing includes following operations: \n",
        "\n",
        "  loading images --> cropping valid areas --> image normalisation --> seperating traing set and testing set(9:1 in size), and finally saving datasets.\n",
        "\n",
        "\n",
        "* Dataset introduction:\n",
        "\n",
        "    We have the [CT images in github](https://github.com/XinjianOUYANG/UNet-for-tumor-segmentation/tree/master/train) of 108 patients, and each patient has 20-30 images. Moreover, the location of tumor of each image is labelled in advance in [lable folder](https://github.com/XinjianOUYANG/UNet-for-tumor-segmentation/tree/master/label)(if no tumor, the image is labelled in black). \n",
        "    \n",
        "* Data cropping\n",
        "\n",
        "    We cropped images in order to extract the areas where the tumor would happen and improve the traning speed.\n",
        "\n",
        "\n",
        "The data is downloaded from a open [competition](http://www.tipdm.org/bdrace/tzjingsai/20181226/1544.html) in China"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo_qfEZL-sW1"
      },
      "source": [
        "### Network Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKKjsELEFYMy"
      },
      "source": [
        "Based on the acknowledge about U-Nets, we construct a U-Nets network using the *keras* package in the environment *tensorflow*. \n",
        "\n",
        "#### Convolutional Block\n",
        "Firstly, we define a convulitonal function which would be used repeatedly. The general structure is: \n",
        "\n",
        "convolutional layer -> standardization layer -> activation function -> convolutional layer -> standardization layer -> activation function -> return result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLfV9w1IH57v"
      },
      "source": [
        "def conv2d_block(input_tensor, n_filters=16, kernel_size=3, batchnorm=True, padding='same'):\n",
        "    # the first layer\n",
        "    x = Conv2D(n_filters, kernel_size, padding=padding)(\n",
        "        input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x) # relu activation function\n",
        "\n",
        "    # the second layer\n",
        "    x = Conv2D(n_filters, kernel_size, padding=padding)(x)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    X = Activation('relu')(x)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GwwDTtBIwvy"
      },
      "source": [
        "#### Contracting Path\n",
        "As shown in the structure graph of U-Nets, the main process of the contracting path is:\n",
        "\n",
        "Input image file -> 1 (convolutional block -> Max Pooling layer -> Dropout layer) -> 2  (convolutional block -> Max Pooling layer -> Dropout layer) -> 3 (convolutional block -> Max Pooling layer -> Dropout layer) -> 4 (convolutional block -> Max Pooling layer -> Dropout layer)\n",
        "\n",
        "Logically, the code is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdmI6J-CKDvI"
      },
      "source": [
        "# Construct an input\n",
        "img = Input(shape=self.shape) # self.shape refers to the dimension of the image\n",
        "\n",
        "# contracting path\n",
        "c1 = conv2d_block(img, n_filters=n_filters * 1, kernel_size=3, batchnorm=batchnorm, padding=padding)\n",
        "p1 = MaxPooling2D((2, 2))(c1)\n",
        "p1 = Dropout(dropout * 0.5)(p1)\n",
        "\n",
        "c2 = conv2d_block(p1, n_filters=n_filters * 2, kernel_size=3, batchnorm=batchnorm, padding=padding)\n",
        "p2 = MaxPooling2D((2, 2))(c2)\n",
        "p2 = Dropout(dropout)(p2)\n",
        "\n",
        "c3 = conv2d_block(p2, n_filters=n_filters * 4, kernel_size=3, batchnorm=batchnorm, padding=padding)\n",
        "p3 = MaxPooling2D((2, 2))(c3)\n",
        "p3 = Dropout(dropout)(p3)\n",
        "\n",
        "c4 = conv2d_block(p3, n_filters=n_filters * 8, kernel_size=3, batchnorm=batchnorm, padding=padding)\n",
        "p4 = MaxPooling2D((2, 2))(c4)\n",
        "p4 = Dropout(dropout)(p4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVD5SFYyKjZP"
      },
      "source": [
        "We finish the contracting path and get a medium layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP9Wtx-aK4nr"
      },
      "source": [
        "c5 = conv2d_block(p4, n_filters=n_filters * 16, kernel_size=3, batchnorm=batchnorm, padding=padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Ewe9-GLKql"
      },
      "source": [
        "#### Expanding Path\n",
        "Similarly, the procedure of the expanding path is: \n",
        "\n",
        "Input from last layer -> 6 (upsampling -> caracteristics combination -> dropout layer -> convolutional block) -> 7 (upsampling -> caracteristics combination -> dropout layer -> convolutional block) -> 8 (upsampling -> caracteristics combination -> dropout layer -> convolutional block) -> 9 (upsampling -> caracteristics combination -> dropout layer -> convolutional block).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY-KLEEMMEMM"
      },
      "source": [
        "# extending path\n",
        "u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides=(2, 2), padding='same')(c5)\n",
        "u6 = concatenate([u6, c4])\n",
        "u6 = Dropout(dropout)(u6)\n",
        "c6 = conv2d_block(u6, n_filters=n_filters * 8, kernel_size=3, batchnorm=batchnorm, padding=padding)\n",
        "\n",
        "u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides=(2, 2), padding='same')(c6)\n",
        "u7 = concatenate([u7, c3])\n",
        "u7 = Dropout(dropout)(u7)\n",
        "c7 = conv2d_block(u7, n_filters=n_filters * 4, kernel_size=3, batchnorm=batchnorm, padding=padding)\n",
        "\n",
        "u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides=(2, 2), padding='same')(c7)\n",
        "u8 = concatenate([u8, c2])\n",
        "u8 = Dropout(dropout)(u8)\n",
        "c8 = conv2d_block(u8, n_filters=n_filters * 2, kernel_size=3, batchnorm=batchnorm, padding=padding)\n",
        "\n",
        "u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides=(2, 2), padding='same')(c8)\n",
        "u9 = concatenate([u9, c1])\n",
        "u9 = Dropout(dropout)(u9)\n",
        "c9 = conv2d_block(u9, n_filters=n_filters * 1, kernel_size=3, batchnorm=batchnorm, padding=padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx76ufM5MUTE"
      },
      "source": [
        "#### Output Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJuRf9kxMh3Z"
      },
      "source": [
        "output = Con2D(1,(1,1), activation = 'sigmoid')(c9)\n",
        "\n",
        "return Model(img, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8IJ190U_LvV"
      },
      "source": [
        "### Compiling Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUFbvlCxGxwj"
      },
      "source": [
        "Here we compile the U-net by setting its basic parameters such as the optimiser, learning rate, loss function and evoluation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfudAtRQVckb"
      },
      "source": [
        "class U_Net():\n",
        "    def __init__(self):\n",
        "        # set the basic parameters of an image\n",
        "        self.height = 256\n",
        "        self.width = 256\n",
        "        self.channels = 1\n",
        "        self.shape = (self.height, self.width, self.channels)\n",
        "        # create a u-net\n",
        "        self.unet = self.build_unet()  \n",
        "        # set Adam as optimizer\n",
        "        optimizer = Adam(0.002, 0.5)\n",
        "        # u_net\n",
        "        self.unet.compile(loss='mse',\n",
        "                          optimizer=optimizer,\n",
        "                          metrics=[self.metric_fun]) # evaluation function\n",
        "        self.unet.summary()\n",
        "    \n",
        "    # use Dice coefficient to evaluation the quality of trained model\n",
        "    def metric_fun(self, y_true, y_pred):\n",
        "        fz = tf.reduce_sum(2 * y_true * tf.cast(tf.greater(y_pred, 0.1), tf.float32)) + 1e-8\n",
        "        fm = tf.reduce_sum(y_true + tf.cast(tf.greater(y_pred, 0.1), tf.float32)) + 1e-8\n",
        "        return fz / fm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijWWXzFyVckb"
      },
      "source": [
        "In our U-net, we use Adam optimiser, MSE(Mean Squared Error) loss function and [Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient), which is often used to evaluate the quality of picture segmentation.\n",
        "\n",
        "**Dice coeffient formula**:\n",
        "![Dice](https://wikimedia.org/api/rest_v1/media/math/render/svg/a80a97215e1afc0b222e604af1b2099dc9363d3b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTuvPDPE_QAu"
      },
      "source": [
        "### Training Network\n",
        "We set the epoch number to 175, and trained an model with the *trian function* below. The model we trained is saved in the folder \"weights\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ICreILV7S3"
      },
      "source": [
        "def train(self, epochs=175, batch_size=32):\r\n",
        "        os.makedirs('./weights', exist_ok=True)\r\n",
        "        # obtain the data\r\n",
        "        x_train, x_label, y_train, y_label = self.load_data()\r\n",
        "\r\n",
        "        # self.unet.load_weights(r\"./best_model.h5\")\r\n",
        "\r\n",
        "        # set checkpoint\r\n",
        "        callbacks = [EarlyStopping(patience=100, verbose=2),\r\n",
        "                     ReduceLROnPlateau(factor=0.5, patience=20, min_lr=0.00005, verbose=2),\r\n",
        "                     ModelCheckpoint('./weights/best_model.h5', verbose=2, save_best_only=True)]\r\n",
        "\r\n",
        "        # start the training\r\n",
        "        results = self.unet.fit(x_train, x_label, batch_size=batch_size, epochs=epochs, verbose=2,\r\n",
        "                                callbacks=callbacks, validation_split=0.1, shuffle=True)\r\n",
        "\r\n",
        "        # plot loss curve\r\n",
        "        loss = results.history['loss']\r\n",
        "        val_loss = results.history['val_loss']\r\n",
        "        metric = results.history['metric_fun']\r\n",
        "        val_metric = results.history['val_metric_fun']\r\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\r\n",
        "        x = np.linspace(0, len(loss), len(loss))  \r\n",
        "        plt.subplot(121), plt.plot(x, loss, x, val_loss)\r\n",
        "        plt.title(\"Loss curve\"), plt.legend(['loss', 'val_loss'])\r\n",
        "        plt.xlabel(\"Epochs\"), plt.ylabel(\"loss\")\r\n",
        "        plt.subplot(122), plt.plot(x, metric, x, val_metric)\r\n",
        "        plt.title(\"metric curve\"), plt.legend(['metric', 'val_metric'])\r\n",
        "        plt.xlabel(\"Epochs\"), plt.ylabel(\"Dice\")\r\n",
        "        plt.show()  \r\n",
        "        fig.savefig('./evaluation/curve.png', bbox_inches='tight', pad_inches=0.1)  #save the graph\r\n",
        "        plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxl4AFFgWWEo"
      },
      "source": [
        "It takes around 30 hours to finish the training by CPU, so we don't recommend you to train it again in this session.\n",
        "\n",
        "And the training result is shown here:\n",
        "\n",
        "![Training Result](https://github.com/XinjianOUYANG/UNet-ST7/blob/master/curve.png?raw=ture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bdnhsxv_V0z"
      },
      "source": [
        "### Test and Evaluation\n",
        "We mainly take the two following facotrs into considertion to evaluate the effect of this model:\n",
        "\n",
        "1.   Dice Coefficient\n",
        "2.   Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaheJFoDgEgv"
      },
      "source": [
        "#### 1.Dice Coefficient\n",
        "We can achive at around **0.8155**, which well proves the similarity between the rumor area segmented by this model and the area drawn by the doctor. \n",
        "\n",
        "![Dice Coefficient](https://github.com/XinjianOUYANG/UNet-ST7/blob/master/Dice_Coefficient.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGI4D6d8gLPm"
      },
      "source": [
        "#### 2.Accuracy\n",
        "The definition of accuracy in this model is very simple:\n",
        "$$\n",
        "acc = \\frac{n_{correct}}{n_{total}}\n",
        "$$\n",
        "And we achieved an accuracy at **99.44%**, which demonstrates that this model works very well. \n",
        "\n",
        "![Accuracy](https://github.com/XinjianOUYANG/UNet-ST7/blob/master/Accuracy.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxbv2gf1gP_7"
      },
      "source": [
        "#### 3.Result Presentation\n",
        "We take one example from the dataset to present the result.\n",
        "\n",
        "The picture in the left is realized by our segmentation model; and the picture in the right is drawn by the doctor. As you can see, the effect is very successful. \n",
        "\n",
        "![](https://github.com/XinjianOUYANG/UNet-ST7/blob/master/evaluation/test_results_Molin/107.png?raw=true)\n",
        "![](https://github.com/XinjianOUYANG/UNet-ST7/blob/master/evaluation/test_results_Molin/107true.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_31NqDjN1Ay"
      },
      "source": [
        "## Little Quiz \n",
        "\n",
        "### Question 1:\n",
        "Why does it exist expansive methods (the grey arrows in the structure graph of U-Nets)? \n",
        "\n",
        "### Question 2:\n",
        "Why is U-Nets method suitable for medicla iamge segmentation? What kinds of caracteristics dedermine? \n",
        "\n",
        "### Question 3:\n",
        "Fill in the three *code tasks* above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT2ixjUe_L0j"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_UQxzO8-KXY"
      },
      "source": []
    }
  ]
}
